这里调用了清华大学专用的模型量化函数，如图18 - 8所示，具体内容请参考本书配套的源码库文件/第十八章/huggingface_saver/quantization.py。

![image](https://github.com/user-attachments/assets/2683c7d3-d136-45df-a22c-597b7faee412)


### 图18 - 8 清华大学专用的模型量化函数
```python
for layer in model.layers:
    layer.attention.query_key_value = QuantizedLinear(
        weight_bit_width=weight_bit_width,
        weight_tensor=layer.attention.query_key_value.weight.to(torch.cuda.current_device()),
        bias_tensor=layer.attention.query_key_value.bias,
        in_features=layer.attention.query_key_value.in_features,
        out_features=layer.attention.query_key_value.out_features,
        bias=True,
        dtype=torch.half,
        device=layer.attention.query_key_value.weight.device,
        empty_init=empty_init
    )
    layer.attention.dense = QuantizedLinear(
        weight_bit_width=weight_bit_width,
        weight_tensor=layer.attention.dense.weight.to(torch.cuda.current_device()),
        bias_tensor=layer.attention.dense.bias,
        in_features=layer.attention.dense.in_features,
        out_features=layer.attention.dense.out_features,
        bias=True,
        dtype=torch.half,
        device=layer.attention.dense.weight.device,
        empty_init=empty_init
    )
    layer.mlp.dense_h_to_4h = QuantizedLinear(
        weight_bit_width=weight_bit_width,
        weight_tensor=layer.mlp.dense_h_to_4h.weight.to(torch.cuda.current_device()),
        bias_tensor=layer.mlp.dense_h_to_4h.bias,
        in_features=layer.mlp.dense_h_to_4h.in_features,
        out_features=layer.mlp.dense_h_to_4h.out_features,
        bias=True,
        dtype=torch.half,
        device=layer.mlp.dense_h_to_4h.weight.device,
        empty_init=empty_init
    )
```
可以看到，源码中实际上是根据设定的量化值大小，分别对各个不同层进行参数量化处理，之后重新将量化后的参数整合在一起。在这里不再深入讲解，有兴趣的读者可自行研究相关内容。

下面有两种实现INT8和INT4的量化方法，即在下载数据的同时完成量化以及构建自定义的量化函数。

（1）**下载数据的同时完成量化**：

```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
.quantize(8).cuda()
```

（2）**构建自定义的量化函数**：

```python
model = xiaohua_model.XiaohuaModel(model_path="../huggingface_saver/chatglm6b.pth", config=config, strict=False)
model.glm_model.quantize(8,False)
model = model.half().cuda()
```

自定义量化函数的使用也较为简单，在这里直接调用模型文件中提供的量化函数即可完成对模型的量化。



需要提醒读者的是，如果使用量化数据进行整体存档，在存档时需要将模型重新定义成同样的量化数值，具体请读者自行完成。

### 18.3 高级微调方法2——基于LoRA的模型微调
Accelerator库的作用是加速对模型的微调，但是对于模型本身的微调方法并没有进行很好的调整，因此在解决大模型的微调问题上并没有做出根本性的改变。基于此，研究人员提出了一种新的能够在冻结原有大模型训练参数的基础上进行微调的训练方法LoRA（Low-Rank Adaptation of Large Language Models，大语言模型的低阶适应）。本节将主要介绍基于LoRA的模型微调方法。

#### 18.3.1 对ChatGLM进行微调的方法——LoRA
LoRA是清华大学的研究人员为了解决大语言模型微调而开发的一项通用技术。

LoRA的思想很简单（见图18 - 9）：

（1）在原始PLM用（Pre-trained Language Model，预训练语言模型）旁边增加一个旁路，进行降维再升维的操作，用来模拟所谓的intrinsic rank。

（2）训练的时候固定PLM的参数，只训练降维矩阵A与升维矩阵B。而模型的输入输出维度不变，输出时将BA与PLM的参数叠加。

（3）用随机高斯分布初始化A，用0矩阵初始化B，以保证训练的开始此旁路矩阵依然是0矩阵。

![image](https://github.com/user-attachments/assets/215e6fc8-f0ef-4ebb-9e06-3323db96440a)


### 图18 - 9 LoRA的思想

假设要在下游任务微调一个预训练语言模型（例如ChatGLM），则需要更新预训练模型参数，公式如下：

\[
\begin{align*}
W&=W_0+\Delta W\\
W_0&\in R^{k\times k}\\
\Delta W&=B\times A
\end{align*}
\]
其中$W_0$是预训练模型初始化的参数，$\Delta W$（确切地说，是B和A构成的矩阵）是需要更新的参数。可以看出，相对于全局调参的大模型来说，$\Delta W$大小可控，而且很经济。

具体来看，在整个模型的训练过程中，只有$\Delta W$会随之更新，而在向前过程中，$\Delta W$和$W_0$都会乘以相同的输入$x$，最后相加：
\[
H = W_0x+\Delta Wx=W_0x + BAx
\]

LoRA的这种思想有点类似于残差连接，同时使用这个旁路的更新来模拟Full Fine-Tuning（完全微调）的过程。并且，Full Fine-Tuning可以被看作LoRA的特例。同时，在推理过程中，LoRA几乎未引入额外的参数，只需要进行全量参数计算即可。

具体使用时，LoRA与Transformer的结合很简单，仅在QKV Attention的计算中增加一个旁路即可。

![image](https://github.com/user-attachments/assets/1321070c-627b-4a59-9e0a-419ab2dc2cd4)


#### 18.3.2 自定义LoRA的使用方法
在讲解自定义的LoRA结构之前，我们首先介绍一下使用方法。读者可以打开本书配套代码库中的/第十八章/fitunning_lora_xiaohua/minlora文件夹，这里提供了相关代码，如图18 - 10所示。

![image](https://github.com/user-attachments/assets/b9dc18b9-55d6-421b-a011-be150719671d)

![image](https://github.com/user-attachments/assets/96071be7-f14f-4742-8b9e-ba372bcbefaa)


### 图18 - 10 minlora文件夹目录结构

其中的model.py文件是对模型参数进行处理的文件，如图18 - 11所示。

![image](https://github.com/user-attachments/assets/3b7b5385-4bdf-417c-b145-31fccebd6bf6)


### 图18 - 11 model.py文件


```python
LoRAParametrization(nn.Module)
default_lora_config
apply_lora(layer, register=True, merge=False, lora_config)
add_lora(model, lora_config=default_lora_config)
merge_lora(model)
remove_lora(model)
get_parameter_number(model)
```

为了便于使用，其中还提供了对模型参数的计算和表示，这是为了显式展示可训练和不可训练的参数。这部分代码如下：

```python
def print_trainable_parameters(model):
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        num_params = param.numel()
        if num_params != 0 and hasattr(param, "ds_numel"):
            num_params = param.ds_numel
        all_param += num_params
        if param.requires_grad:
            trainable_params += num_params
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )
```
完整的LoRA使用情况如下：

```python
model = xiaohua_model.XiaohuaModel(model_path="../huggingface_saver/chatglm6b.pth", config=config, strict=False)
model = model.half().cuda()

for name,param in model.named_parameters():
    param.requires_grad = False

#下面就是LoRA的部分
from minlora.model import *
from minlora.utils import *

for key,_layer in model.named_modules():
    if "query_key_value" in key:
        add_lora(_layer)
xiaohua_model.print_trainable_parameters(model)
```

可以看到，在这里首先完成了对模型的载入，之后通过对所有参数的调整，将所有参数设置为无法求导，这部分是不可训练的，之后我们选取了特定层，即对模型中名为query_key_value的部分完成LoRA的注入。最终打印参数结构如图18 - 12所示。

### 图18 - 12 打印参数结构
```
trainable params: 1835008 || all params: 6709764096 || trainable%: 0.027348323633224796
```

![image](https://github.com/user-attachments/assets/f8616250-9c99-45f1-8db2-bdeba4051629)


#### 18.3.3 基于自定义LoRA的模型训练

LoRA对模型的修正，实际上就是冻结原有的模型参数，从而只训练插入的参数。因此，有多种方式对参数进行保存：

（1）保存所有的参数文件。

（2）仅保存LoRA更新的参数。

保存所有参数文件的方法这里不再举例，使用PyTorch 2.0标准模型保存方法即可，而对于仅保存LoRA更新的参数的方法，可以通过如下代码读取对应的参数：
```python
def name_is_lora(name):
    return (
        len(name.split(".")) >= 4
        and name.split(".")[-4] == "parametrizations"
        and name.split(".")[-1] in ["lora_A", "lora_B"]
    )

def get_lora_state_dict(model):
    return {k: v for k, v in model.state_dict().items() if name_is_lora(k)}
```

完整的基于LoRA的模型训练代码如下：

```python
import torch
from transformers import AutoTokenizer
from torch.utils.data import RandomSampler, DataLoader
from 第十八章.chatGLM_spo.huggingface_saver import xiaohua_model, configuration_chatglm, modeling_chatglm
from tqdm import tqdm

config = configuration_chatglm.ChatGLMConfig()
#这里设置config中的pre_seq_len与prefix_projection，只有这2项设置好了才行

model = xiaohua_model.XiaohuaModel(model_path="../huggingface_saver/chatglm6b.pth", config=config, strict=False)
model = model.half().cuda()

for name,param in model.named_parameters():
    param.requires_grad = False

# 下面就是LoRA的部分
from minlora.model import *
from minlora.utils import *

for key,_layer in model.named_modules():
    if "query_key_value" in key:
        add_lora(_layer)
xiaohua_model.print_trainable_parameters(model)

prompt_text = "按给定的格式抽取文本信息。\n文本："
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
from 第十八章.chatGLM_spo import get_data
all_train_data = get_data.get_train_data("../data/spo_0.json", tokenizer, 128, 96,prompt_text)
train_dataset = get_data.Seq2SeqDataSet(all_train_data)
train_loader = DataLoader(train_dataset, batch_size=2, drop_last=True, collate_fn=get_data.coll_fn, num_workers=0)

from accelerate import Accelerator
accelerator = Accelerator()
device = accelerator.device

lora_parameters = [{"params": list(get_lora_params(model))}]
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-5)
lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2400, eta_min=2e-6, last_epoch=-1)

model, optim, train_loader, lr_scheduler = accelerator.prepare(model, optimizer, train_loader, lr_scheduler)

for epoch in range(96):
    pbar = tqdm(train_loader, total=len(train_loader))
    for batch in (pbar):
        input_ids = batch["input_ids"].cuda()
        labels = batch["labels"].cuda()

        _,loss = model.forward(input_ids, labels=labels)
        accelerator.backward(loss)
        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)
        optimizer.step()
        lr_scheduler.step()  # 执行优化器
        optimizer.zero_grad()

        pbar.set_description(
            f"epoch:{epoch + 1}, train_loss:{loss.item():.5f}, lr:{lr_scheduler.get_last_lr()[0] * 1000:.5f}"
        )

        torch.save(model.state_dict(), "./glm6b_lora_all.pth")
        lora_state_dict = get_lora_state_dict(model)
        torch.save(lora_state_dict, "./glm6b_lora_only.pth")
```
可以看到，这里使用了两种不同的方法对模型的参数进行保存，即仅保存LoRA更新的参数与保存所有的参数，具体读者可以在训练完毕后自行对比查看。

还有一个需要注意的地方，在编写优化函数的时候，是对全部参数直接进行优化，虽然这里对模型原有的参数进行了冻结操作，但是整体在训练时同样也要耗费大量的计算空间。因此，这里提供了专门提取LoRA训练参数的方法，代码如下：

```python
def get_params_by_name(model, print_shapes=False, name_filter=None):
    for n, p in model.named_parameters():
        if name_filter is None or name_filter(n):
            if print_shapes:
                print(n, p.shape)
            yield p

def get_lora_params(model, print_shapes=False):
    return get_params_by_name(model, print_shapes=print_shapes, name_filter=name_is_lora)
```

在具体使用时，我们可以优化代码如下：

```python
lora_parameters = [{"params": list(get_lora_params(model))}]
optimizer = torch.optim.AdamW(lora_parameters.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-5)
```
请读者自行验证。

#### 18.3.4 基于自定义LoRA的模型推断
本小节基于自定义的LoRA参数对数据进行推断。

如果此时我们想要保存全体数据的话，那么可以直接使用PyTorch 2.0的参数载入方法完成对整体参数的载入，实现代码如下：
```python
#注意此时的strict设置成False
model = xiaohua_model.XiaohuaModel(model_path="../huggingface_saver/chatglm6b.pth", config=config, strict=False)
# 下面就是LoRA的部分
from minlora.model import *
from minlora.utils import *

for key,_layer in model.named_modules():
    if "query_key_value" in key:
        add_lora(_layer)
for name,param in model.named_parameters():
    param.requires_grad = False
model.load_state_dict(torch.load("./glm6b_lora.pth"))
```
除此之外，对于自定义LoRA参数的载入方法，此时需要注意的是，在模型参数载入的过程中，strict被设置成False，这是按模型的载入方法非严格地载入模型参数，通过原有的add_lora函数将LoRA参数重新加载到模型中。代码如下：
```python
for key,_layer in model.named_modules():
    if "query_key_value" in key:
        add_lora(_layer)
for name,param in model.named_parameters():
    param.requires_grad = False
model.load_state_dict(torch.load("./glm6b_lora_only.pth"), strict = False)
model = model.half().cuda()
```
最终决定采用哪种方式，需要读者根据自身的文档保存结果自行决定。一个完整的使用推断代码实现的例子如下：
```python
def sample_top_p(probs, p):
    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)
    probs_sum = torch.cumsum(probs_sort, dim=-1)
    mask = probs_sum - probs_sort > p
    probs_sort[mask] = 0.0
    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))
    next_token = torch.multinomial(probs_sort, num_samples=1)
    next_token = torch.gather(probs_idx, -1, next_token)
    return next_token

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
import torch
from transformers import AutoTokenizer
from torch.utils.data import RandomSampler, DataLoader
from 第十八章.chatGLM_spo.huggingface_saver import xiaohua_model, configuration_chatglm, modeling_chatglm
from tqdm import tqdm

config = configuration_chatglm.ChatGLMConfig()
#这里设置config中的pre_seq_len与prefix_projection，只有这2项设置好了才行
``` 
