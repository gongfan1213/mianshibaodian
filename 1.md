### 第18章 对训练成本上亿美元的ChatGLM进行高级微调

第17章带领读者学习了ChatGLM的高级应用，相信读者对如何使用ChatGLM完成一些简单或者较高级的任务有了一定的了解。

但是第17章讲解的所有任务和应用场景均是以现有的模型训练本身的结果为基础的，没有涉及针对ChatGLM本身的源码修改和微调方面的内容。本章将学习这方面的内容。



#### 18.1 ChatGLM模型的本地化处理

可能有读者注意到，到目前为止我们使用的都是在线版本的ChatGLM，即通过Transformer这个工具包直接调用相关接口，如果我们想离线使用ChatGLM，是否可行呢？

答案是可以的，本节将解决这个问题，即实现ChatGLM模型的本地化。



##### 18.1.1 下载ChatGLM源码与合并存档

在Huggingface的ChatGLM对应的库内容下，清华大学相关实验室为用户提供了对应的ChatGLM源码下载，如图18 - 1所示。

其中的modeling_chatglm.py文件就是供用户下载和学习的源码内容，读者可以下载当前文件夹中除了存档文件（以.bin后缀结尾的文件）之外的所有文件备用。

![image](https://github.com/user-attachments/assets/ea9b04f5-9140-4e08-8632-4e7f13f386f3)


### 图18-1 ChatGLM源码下载页面

在这里，读者可以直接使用本书配套代码包中的chatGLM_spo/huggingface_saver/demo文件夹，从而完成对存档内容的读取与载入。完整的代码如下：

```python
import torch
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
#为了节省显存，我们使用的是half数据格式，也就是半精度的模型
model = AutoModel.from_pretrained("THUDM/chatglm-6b",
trust_remote_code=True).half().cuda()
response, history = model.chat(tokenizer, "你好", history=[])
print(response)
# 你好！我是人工智能助手ChatGLM-6B，很高兴见到你，欢迎问我任何问题
response, history = model.chat(tokenizer, "晚上睡不着应该怎么办", history=history)
print(response)
torch.save(model.state_dict(),"./huggingface_saver/chatglm6b.pth")
```

需要注意的是，对于模型读取，这里使用的是半精度的模型，也就是模型的half数据格式，这是为了节省显存而采用的一种方式，如果读者有条件，可以删除half()函数直接使用全精度。而使用全精度形式可能会要求读者重新下载模型存档文件。相对于半精度的模型，全精度的模型存档占用磁盘空间较大，具体使用哪个模型读者可自行斟酌。

当读者运行完此段代码后，查询chatGLM_spo/huggingface_saver文件夹中的内容可以看到，文件夹中生成了对应的ChatGLM存档文件，相对于前期分散的存档来说，此时的存档文件是一个单独的带有.pth后缀的文件，如图18 - 2所示。

![image](https://github.com/user-attachments/assets/2ce4edbf-2fa7-44c6-9c88-77ee369204b8)



### 图18-2 存档文件

下面回到ChatGLM的模型文件上，此时应该可以打开本书配套代码包中chatGLM_spo/huggingface_saver/modeling_chatglm.py文件，其构成如图18 - 3所示。

![image](https://github.com/user-attachments/assets/a4b0bdfd-836c-46e1-8bae-2d22221d6487)


### 图18-3 modeling_chatglm.py文件

这里展示的是已处理好的ChatGLM文件内容，相对于原始的modeling_chatglm，这里的文件删除了直接的chat内容，以及负责生成文本的ChatGLMForConditionalGeneration类的训练部分，只留下了负责文本生成的全连接层head。

```python
self.lm_head = skip_init(
    nn.Linear,
    config.hidden_size,
    config.vocab_size,
    bias=False,
    dtype=torch.half
)
```

我们仅仅需要知道最后一个lm_head的作用是对生成的隐变量进行映射，将输出的隐变量映射到对应的字符Embedding上即可。

至此，我们完成了对ChatGLM进行本地化处理的第一步，即ChatGLM主体文件的本地化。读者可以采用如下代码，尝试使用上面示例存储的.pth文件完整实现ChatGLM，代码如下：
```python
model_path = "./chatglm6b.pth"
#config文件在代码段下方解释
glm_model = modeling_chatglm.ChatGLMForConditionalGeneration(config)
model_dict = torch.load(model_path)
```

其中config文件用于对ChatGLM文件进行主体化设置，其主要内容如下：

```python
def __init__(
    self,
    vocab_size=130528,  #字符个数
    hidden_size=4096,  #隐变量维度大小
    num_layers=28,  #模型深度
    num_attention_heads=32,  #模型问数
    layernorm_epsilon=1e-5,  #layernorm的极值
    use_cache=False,  #是否使用缓存
    bos_token_id=130004,  #序列起始字符编号
    eos_token_id=130005,  #序列结束字符编号
    mask_token_id=130000,  #mask字符的编号
    gmask_token_id=130001,  #自回归mask字符编号
    pad_token_id=0,  #padding字符编号
    max_sequence_length=2048,  #模型生成的最大字符长度
    inner_hidden_size=2048,  #模型forward的维度大小
    position_encoding_2d=True,  #使用2d位置编码
    quantization_bit=0,  #模型量化参数，默认为0，即使用模型量化处理
    pre_seq_len=None,  #预输入的序列长度
    ...
):
```
代码中定义了相当多的参数，vocab_size是模型输入的字符数量，num_layers设置的是模型的深度，max_sequence_length是模型输入输出文本长度。在这里通过对参数的设定确定了模型的主体大小和处理要求。

##### 18.1.2 修正自定义的本地化模型

本小节修正自定义的本地化模型，将模型设置成本地可以处理并熟悉的内容，完整代码如下：

```python
import torch
from 第十八章.chatGLM_spo.huggingface_saver import configuration_chatglm,modeling_chatglm

class XiaohuaModel(torch.nn.Module):
    def __init__(self,model_path = "./chatglm6b.pth",config = None,strict = True):
        super().__init__()
        self.glm_model = modeling_chatglm.ChatGLMForConditionalGeneration(config)
        model_dict = torch.load(model_path)
        self.glm_model.load_state_dict(model_dict,strict = strict)
        self.loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)
    def forward(self,input_ids,labels = None,position_ids = None,attention_mask = None):
        logits,hidden_states = self.glm_model.forward(input_ids=input_ids,position_ids = None,attention_mask = None)
        loss = None
        if labels != None:
            shift_logits = logits[:, :-1, :].contiguous()
            shift_labels = input_ids[:, 1:].contiguous()
            # Flatten the tokens
            logits_1 = shift_logits.view(-1, shift_logits.size(-1))
            logits_2 = shift_labels.view(-1)
            loss = self.loss_fct(logits_1, logits_2)
        return logits,hidden_states,loss
    def generate(self,start_question_text="抗原呈递的原理是什么？",continue_seq_length = 128,tokenizer = None,temperature = 0.95, top_p = 0.95):
        """
        Args:
            start_question_text:这里指的是起始问题，需要用中文进行展示
            continue_seq_length: 这里是在question后面需要添加的字符
            temperature:
            top_p:
        Returns:
        """
        #记录：这个tokenizer可能会在开始执行encode的时候，在最开始加上一个空格20005
        if not history:
            prompt = query
        else:
            prompt = ""
            for i, (old_query, response) in enumerate(history):
                prompt += "[Round {}]\n问: {}\n答: {}\n".format(i, old_query, response)
            prompt += "[Round {}]\n问: {}".format(len(history), query)
        """
        #这里是一个简单的例子，用来判定是问答还是做其他工作
        if "：" not in start_question_text:
            inputs_text_ori = start_question_text
            inputs_text = f"[Round 0]\n问: {inputs_text_ori}\n答: "
        else:
            inputs_text = start_question_text
        input_ids = tokenizer.encode(inputs_text)
        for _ in range(continue_seq_length):
            input_ids_tensor = torch.tensor(input_ids).to("cuda")
            logits,_ = self.forward(input_ids_tensor)
            logits = logits[:,-3:]
            probs = torch.softmax(logits / temperature, dim=-1)
            next_token = self.sample_top_P(probs, top_p)  # 预设的top_p = 0.95
            #next_token = next_token.reshape(-1)
            input_ids = input_ids[:-2] + [next_token.item()] + input_ids[-2:]
            if next_token.item() == 130005:
                print("break")
                break
            result = tokenizer.decode(input_ids)
            return result
        def sample_top_p(self,probs, p):
            probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)
            probs_sum = torch.cumsum(probs_sort, dim=-1)
            mask = probs_sort > p
            probs_sort[mask] = 0.0
            probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))
            next_token = torch.multinomial(probs_sort, num_samples=1)
            return torch.gather(probs_idx, -1, next_token)
```
在这里使用基础的ChatGLMForConditionalGeneration作为主处理模型，之后对lm_head输出的内容逐一进行迭代生成。具体可以参考本书14.1.3节中对GPT生成函数的说明。
下面检测一下模型的输出，代码如下：
```python
if __name__ == '__main__':
    from transformers import AutoTokenizer
    import tokenization_chatglm
    config = configuration_chatglm.ChatGLMConfig()
    model = XiaohuaModel(config=config).half().cuda()
    tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b",
    trust_remote_code=True,cache_dir = "./huggingface_saver")
    inputs_text_ori = "抗原呈递的原理是什么？"
    result = model.generate(inputs_text_ori,
    continue_seq_length=256,tokenizer=tokenizer)
    print(result)
    while True:
        print("请输入:")
        ques = input()
        inputs_text_ori = ques
        result = model.generate(inputs_text_ori, continue_seq_length=256,
        tokenizer=tokenizer)
        print(result)
```
为了考验模型的输出能力，在这里我们提出了一个较为专业的问题（医学常识“抗原呈递”方面的内容）对其进行考察，结果请读者自行查阅。

为了便于测试，代码中实现了一个while循环用于接收用户输入的文本内容，这样可以使得读者在测试文本时不用多次重启模型。更多的内容请读者自行尝试并输出对应的文本。

另外需要注意的是，在完整的模型中还集成了loss方面的计算，即常用的交叉熵损失函数的计算。这里将其与模型本身集成在一起的原因在于：对于分布式计算，损失函数如果过于集中，就会造成主硬件的负载过大，而将损失函数的计算分配到每个单独的模型实例中，可以较好地降低损失函数的计算成本。



##### 18.1.3 构建GLM模型的输入输出示例

相对于传统的GPT模型，GLM模型的创新点主要在于输入输出数据结合了自编码和自回归的输入输出形式：

- 自编码：随机MASK输入中连续跨度的Token。

- 自回归：结合前文内容（包括自编码的mask部分），去预测下一个输出值。

结合了自编码与自回归的GLM训练模式如图18 - 4所示。 
