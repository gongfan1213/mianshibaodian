```python
model = xiaohua_model.XiaohuaModel(model_path="../huggingface_saver/chatglm6b.pth", config=config, strict=False)

# 下面就是LoRA的部分
from minlora.model import *
from minlora.utils import *

for key, _layer in model.named_modules():
    if "query_key_value" in key:
        add_lora(_layer)
for name, param in model.named_parameters():
    param.requires_grad = False
#model.load_state_dict(torch.load("./glm6b_lora.pth"))  # 加载保存的全部数据存档
# 加载LoRA存档
model.load_state_dict(torch.load("./glm6b_lora_only.pth"), strict=False)
model = model.half().cuda()

xiaohua_model.print_trainable_parameters(model)
model.eval()
max_len = 288
max_src_len = 256
prompt_text = "按给定的格式抽取文本信息。\n文本："
save_data = []
f1 = 0.0
max_tgt_len = max_len - max_src_len - 3
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
import time, json
s_time = time.time()
with open("../data/spo_0_1.json", "r", encoding="utf-8") as fh:
    for i, line in enumerate(tqdm(fh, desc="iter")):
        with torch.no_grad():
            sample = json.loads(line.strip())
            src_tokens = tokenizer.tokenize(sample["text"])
            prompt_tokens = tokenizer.tokenize(prompt_text)
            if len(src_tokens) > max_src_len - len(prompt_tokens):
                src_tokens = src_tokens[:max_src_len - len(prompt_tokens)]
            tokens = prompt_tokens + src_tokens + ["[gMASK]", "<sop>"]
            input_ids = tokenizer.convert_tokens_to_ids(tokens)
            for _ in range(max_src_len):
                input_ids_tensor = torch.tensor([input_ids]).to("cuda")
                logits, _, _ = model.forward(input_ids_tensor)
                logits = logits[:, -3:]
                probs = torch.softmax(logits / 0.95, dim=-1)
                next_token = sample_top_p(probs, 0.9)  # 预设的top_p = 0.9
                # next_token = next_token.reshape(-1)
                # next_token = result_token[-3:-2]
                input_ids = input_ids[:-2] + [next_token.item()] + input_ids[-2:]
                if next_token.item() == 130005:
                    print("break")
                    break
                result = tokenizer.decode(input_ids)
                print(result)
                print("----------------------")
```
上面代码提供了两种对LoRA参数进行载入的方法，即全量载入和只载入特定的LoRA参数。具体采用哪种方式，需要读者自行考虑。

### 18.3.5 基于基本原理的LoRA实现
对于LoRA的基本原理和具体使用方法，前面已经做了完整的讲解。LoRA实际上就是使用一个额外的网络加载到对应的位置上，从而完成对模型预测结果的修正。LoRA参数载入的实现代码如下：
```python
class LoRAParametrization(nn.Module):
    def __init__(self, fan_in, fan_out, fan_in_fan_out=False, rank=4, lora_dropout_p=0.0, lora_alpha=1):
        super().__init__()
        # if weight is stored as (fan_out, fan_in), the memory layout of A & B follows
        # otherwise, it's x(W + AB). This allows us to tie the weights between linear layers
        # and embeddings
        self.swap = (lambda x: (x[1], x[0])) if fan_in_fan_out else (lambda x: x)
        self.lora_A = nn.Parameter(torch.zeros(self.swap((rank, fan_in))))
        self.lora_B = nn.Parameter(torch.zeros(self.swap((fan_out, rank))))
        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
        self.lora_alpha = lora_alpha
        self.scaling = lora_alpha / rank
        self.lora_dropout = nn.Dropout(p=lora_dropout_p) if lora_dropout_p > 0 else lambda x: x
        self.dropout_fn = self._dropout if lora_dropout_p > 0 else lambda x: x
        self.register_buffer("lora_dropout_mask", torch.ones(self.swap((1, fan_in)), dtype=self.lora_A.dtype))
        self.forward_fn = self.lora_forward

    def _dropout(self, A):
        # to mimic the original implementation: A @ dropout(x), we do (A * dropout(ones))
        # @ x
        return A * self.lora_dropout(self.lora_dropout_mask)

    def lora_forward(self, X):
        return X + (torch.mm(*self.swap((self.lora_B, self.scaling).half().to(X.device) * self.dropout_fn(self.lora_A)))).view(X.shape)

    def forward(self, X):
        return self.forward_fn(X)

    def disable_lora(self):
        self.forward_fn = lambda x: x

    def enable_lora(self):
        self.forward_fn = self.lora_forward

    @classmethod
    def from_linear(cls, layer, rank=4, lora_dropout_p=0.0, lora_alpha=1):
        fan_out, fan_in = layer.weight.shape
        return cls(fan_in, fan_out, fan_in_fan_out=False, rank=rank, lora_dropout_p=lora_dropout_p, lora_alpha=lora_alpha)

    @classmethod
    def from_conv2d(cls, layer, rank=4, lora_dropout_p=0.0, lora_alpha=1):
        fan_out, fan_in = layer.weight.view(layer.weight.shape[0], -1).shape
        return cls(fan_in, fan_out, fan_in_fan_out=False, rank=rank, lora_dropout_p=lora_dropout_p, lora_alpha=lora_alpha)

    @classmethod
    def from_embedding(cls, layer, rank=4, lora_dropout_p=0.0, lora_alpha=1):
        fan_in, fan_out = layer.weight.shape
        return cls(fan_in, fan_out, fan_in_fan_out=True, rank=rank, lora_dropout_p=lora_dropout_p, lora_alpha=lora_alpha)
```
这里使用了Python高级编程方法，即通过cls这个特殊的函数将初始化中的参数lora_A、lora_B与原模型中的参数建立并联。

对于不同的模型计算层，需要使用不同的注入方案，下面展示以全连接层为默认目标层完成参数输入的方法。代码如下：
```python
default_lora_config = {
    # specify which layers to add lora to, by default only add to linear layers
    nn.Linear: {
        "weight": partial(LoRAParametrization.from_linear, rank=4),
    },
}

def apply_lora(layer, register=True, merge=False, lora_config=default_lora_config):
    """add lora parametrization to a layer, designed to be used with model.apply"""
    if register:
        if type(layer) in lora_config:
            for attr_name, parametrization in lora_config[type(layer)].items():
                parametrize.register_parametrization(layer, attr_name, parametrization(layer))
        else:  # this will remove all parametrizations, use with caution
            if hasattr(layer, "parametrizations"):
                for attr_name in layer.parametrizations.keys():
                    parametrize.remove_parametrizations(layer, attr_name, leave_parametrized=merge)

def add_lora(model, lora_config=default_lora_config):
    """add lora parametrization to all layers in a model. Calling it twice will add lora twice"""
    model.apply(partial(apply_lora, lora_config=lora_config))

def merge_lora(model):
    """merge lora parametrization to all layers in a model. This will remove all parametrization"""
    model.apply(partial(apply_lora, register=False, merge=True))

def remove_lora(model):
    """remove lora parametrization to all layers in a model. This will remove all parametrization"""
    model.apply(partial(apply_lora, register=False, merge=False))

def get_parameter_number(model):
    total_num = sum(p.numel() for p in model.parameters())
    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return {"total_para_num":total_num,"trainable_para_num":trainable_num}
```
其中add_lora和merge_lora等函数主要用于将定义的LoRA层加载到对应的模型中，remove_lora函数的作用是移除模型中已有的LoRA层。请读者自行验证。

### 18.4 高级微调方法3——基于Huggingface的PEFT模型微调 
