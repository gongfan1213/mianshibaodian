![image](https://github.com/user-attachments/assets/36245e94-3942-4143-85be-7ee65cc401cd)


### 图18-4 结合了自编码与自回归的GLM训练模式

这里解释一下（彩图请参看本书配套源码包中的相关文件）：

（1）左侧图(a)，原始文本是[x1, x2, x3, x4, x5, x6]。两个将采样的跨度是[x3]与[x5, x6]。图中A部分用[M]代替被采样的跨度，打乱B部分的跨度。

（2）中间图(c)，GLM自回归生成B部分，每个跨度以[S]作为输入进行预置，以[E]作为输出进行附加。2D位置代表跨度间和跨度内的位置。

（3）右侧图(d)，x区域被遮掉了。A部分（蓝色框架内）能注意到自己，但不能注意到B部分。B部分的令牌可以关注A及其在B中的前因（黄色和绿色框架对应两个跨度），[M] := [MASK]，[S] := [START]，[E] := [END]。



在这种训练方式下，GLM整合了“自编码器”与“自回归器”，从而完成了模型训练的统一。而其中mask字符的比例一般占全部文本的15%左右。

上面是对原始的GLM训练过程中输入输出的解释，具体在后期的输入输出操作上会有两种输入：

（1）遵循原有设计，带有mask的半错误输入输出匹配如图18 - 5所示。

![image](https://github.com/user-attachments/assets/59058e31-5391-46e7-b5e5-1c546c777307)


### 图18-5 带有mask的半错误输入输出匹配
从图中可以很明显地看到，左侧区域并没有对下一个字符进行预测的操作，而右侧区域则根据输入的字符进行错位输入和输出。

（2）遵循GPT - 2的传统输入输出方式，如图18 - 6所示。

### 图18-6 遵循GPT - 2的传统输入输出方式

这就是较简单的GPT - 2输入输出方法，读者可以查看前面章节的内容来完成。在本书后续的微调过程中，将采用第（2）种方式完成模型的微调训练。

![image](https://github.com/user-attachments/assets/468a7dbe-0d4b-41a2-a4a0-a8d65dd9aa9b)


### 18.2 高级微调方法1——基于加速库Accelerator的全量数据微调

18.1节介绍了使用ChatGLM进行微调的官方方案。需要注意的是，这种微调方法是基于Linux完成的，并且采用脚本的形式来完成（官方的ChatGLM微调）。这种方法简单易行，可以很好地完成既定目标，但是对于学习和掌握相关内容的使用者来说，这种微调方法可能并不是很合适。

特别是当我们采用ChatGLM - 130B的大模型时，将会有1300亿参数，而为了让它能完成特定领域的工作，不会只采用知识链的方法，而需要对其进行微调。但是问题在于，如果直接对ChatGLM - 130B进行微调，成本太高也太麻烦了。对于一般的小公司或者个人来说，想要开发自己的大模型几乎不可能，像ChatGLM这样的大模型，一次训练的成本就达上亿甚至十几亿美元。

针对此情况，本节将向读者演示ChatGLM的高级微调方法，通过演示向读者讲解如何在消费级的显卡和普通的Windows操作系统上完成微调任务。

![image](https://github.com/user-attachments/assets/bb003426-4a70-4e6f-9b1d-538f6698468c)


#### 18.2.1 数据的准备——将文本内容转化成三元组的知识图谱

本小节将实现基于多种方法的ChatGLM的模型微调，无论采用哪种微调方法，都需要准备数据。这里准备了一份基于维修记录的文本数据，内容如图18 - 7所示。

![image](https://github.com/user-attachments/assets/4b41cbe1-5fc4-4872-a420-06f3e9d57a6a)


### 图18-7 基于维修记录的文本数据

可以看到，text中就是出现的问题，而answer中是前面文本内容按关系的抽取。我们要做的是通过问答的内容从中提取涉及“性能故障”，“部件故障”，“组成”和“检测工具”的三元组。

同时，根据任务目标和模型本身的特性，设计一种适配模板，如下所示：

“你现在是一个信息抽取模型，请你帮我抽取出关系内容为“性能故障”，“部件故障”，“组成”和“检测工具”的相关三元组，三元组内部用“\_”连接，三元组之间用\n分隔。文本：”

注意：这是作者设计的标准模板，在具体使用时这个模板会占据大量的可用字符空间，读者可以在后期将其更换为自己需要的内容。

这样设置的Prompt需要对输入的数据结构重新进行调整，并且要符合原有的ChatGLM的输入格式，新的Token字符构成如下：

```python
tokens = prompt_tokens + src_tokens + ["[gMASK]", "<sop>"] + tgt_tokens + ["<eop>"]
```
此时，原有的输入文本被重新分隔并被转化成新的被分隔后的文本，如下所示：
```js

['_', '你现在', '是一个', '信息', '抽取', '模型', '请你', '帮我', '抽取', '出', '关系', '内容为', "'", '性能', '故障', "'", ',', "'", '部件', '故障', "'", ',', "'", '组成', "'", '和', "'", '检测', '工具', "'", '的相关', '三元', '组', '三元', '组', '内部', '用', "'", '_', "'", '连接', ',', '三元', '组', '之间', '用', '\\', 'n', '分隔', '.', '文本', ':', '_', '故障', '现象', ':', '冷', '车', '起动', '困难', ',', '车', '起动', '后', '发动机', '_', '起动', '困难', '<n>', '发', '<sop>', '_', '<n>', '原因', ':', '车', '起动', '部件', '故障', '_', '<n>', '发动机', '_', '部件', '故障', '_', '抖动', '<eop>']
```

从上面可以看到使用了tokenizer.tokenize函数分隔后的文本内容。对文本内容进行处理的函数如下：
```python
def get_train_data(data_path,tokenizer,max_len, max_src_len, prompt_text):
    max_tgt_len = max_len - max_src_len - 3
    all_data = {}
    with open(data_path, "r", encoding="utf-8") as fh:
        for i, line in enumerate(fh):
            sample = json.loads(line.strip())
            src_tokens = tokenizer.tokenize(sample["text"])
            prompt_tokens = tokenizer.tokenize(prompt_text)
            if len(src_tokens) > max_src_len - len(prompt_tokens):
                src_tokens = src_tokens[:max_src_len - len(prompt_tokens)]
            tgt_tokens = tokenizer.tokenize("\n原因:"+sample["answer"])
            if len(tgt_tokens) > max_tgt_len:
                tgt_tokens = tgt_tokens[:max_tgt_len]
            tokens = prompt_tokens + src_tokens + ["[gMASK]", "<sop>"] + tgt_tokens + ["<eop>"]
            input_ids = tokenizer.convert_tokens_to_ids(tokens)
            context_length = input_ids.index(tokenizer.bos_token_id)
            mask_position = context_length - 1
            labels = [-100] * context_length + input_ids[mask_position + 1:]
            pad_len = max_len - len(input_ids)
            input_ids = input_ids + [tokenizer.pad_token_id] * pad_len
            labels = labels + [-100] * pad_len
            all_data.append({
                "text": sample["text"], "answer": sample["answer"], "input_ids": input_ids, "labels": labels
            })
    return all_data
```
这里的tokenizer函数传入的模型初始化编码器，而tokenizer()函数和convert_tokens_to_ids()函数分别完成了参数的切分与转换任务。对于最后labels的设计，需要遵循GLM原有的设计原理，GLM本身是一个自编码与自回归集成在一起的生成性文本模型，因此需要使用这种特殊的编码形式。有兴趣的读者可以自行了解和学习相关的GLM生成模型的相关内容。


完整的数据处理代码如下：
```python
import json
import torch
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import Dataset
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
def get_train_data(data_path,tokenizer,max_len, max_src_len, prompt_text):
    max_tgt_len = max_len - max_src_len - 3
    all_data = []
    with open(data_path, "r", encoding="utf-8") as fh:
        for i, line in enumerate(fh):
            sample = json.loads(line.strip())
            src_tokens = tokenizer.tokenize(sample["text"])
            prompt_tokens = tokenizer.tokenize(prompt_text)
            if len(src_tokens) > max_src_len - len(prompt_tokens):
                src_tokens = src_tokens[:max_src_len - len(prompt_tokens)]
            tgt_tokens = tokenizer.tokenize("\n原因:"+sample["answer"])
            if len(tgt_tokens) > max_tgt_len:
                tgt_tokens = tgt_tokens[:max_tgt_len]
            tokens = prompt_tokens + src_tokens + ["[gMASK]", "<sop>"] + tgt_tokens + ["<eop>"]
            input_ids = tokenizer.convert_tokens_to_ids(tokens)
            context_length = input_ids.index(tokenizer.bos_token_id)
            mask_position = context_length - 1
            labels = [-100] * context_length + input_ids[mask_position + 1:]
            pad_len = max_len - len(input_ids)
            input_ids = input_ids + [tokenizer.pad_token_id] * pad_len
            labels = labels + [-100] * pad_len
            all_data.append({
                "text": sample["text"], "answer": sample["answer"], "input_ids": input_ids, "labels": labels
            })
    return all_data
#服务PyTorch计算的数据输入类
class Seq2SeqDataSet(Dataset):
    """数据处理函数"""
    def __init__(self, all_data):
        self.all_data = all_data
    def __len__(self):
        return len(self.all_data)
    def __getitem__(self, item):
        instance = self.all_data[item]
        return instance
def coll_fn(batch):
    input_ids_list, labels_list = [], []
    for instance in batch:
        input_ids_list.append(torch.tensor(instance["input_ids"], dtype=torch.long))
        labels_list.append(torch.tensor(instance["labels"], dtype=torch.long))
    return {
        "input_ids": pad_sequence(input_ids_list, batch_first=True, padding_value=3),  #这里原来是20003，vocab改成了3
        "labels": pad_sequence(labels_list, batch_first=True, padding_value=3)
    }
if __name__ == '__main__':
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
    all_data = get_train_data("./data/spo_0.json", tokenizer, 768, 450, "你现在是一个信息抽取模型，请你帮我抽取出关系内容为\"性能故障\"，\"部件故障\"，\"组成\"和 \"检测工具\"的相关三元组，三元组内部用\"_\"连接，三元组之间用\n分隔。文本: ")
    train_dataset = Seq2SeqDataSet(all_data)
    instance = train_dataset.__getitem__(0)
    text,ans,input_ids,lab = instance
    print(len(instance["input_ids"]))
    print(len(instance["labels"]))
    from torch.utils.data import RandomSampler, DataLoader
    train_loader = DataLoader(train_dataset, batch_size=4, drop_last=True, num_workers=0)
```
在上面代码中，为了方便PyTorch 2.0载入数据，提供了一个数据载入类Seq2SeqDataSet，从而可以更好地进行模型的计算工作。 
