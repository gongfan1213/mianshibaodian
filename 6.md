### 18.4 高级微调方法3——基于Huggingface的PEFT模型微调
18.3节演示了如何使用自定义LoRA完成对下载的模型进行微调，相对于不同的训练方法，全参数微调是一个更好的选择，但是采用LoRA的方式可以在不损失或者较少损失精度的情况下使得模型更快地收敛。

Huggingface开源了一种基于LoRA的高效微调大模型的库PEFT（Parameter-Efficient Fine-Tuning，高效参数微调），如图18 - 13所示。

![image](https://github.com/user-attachments/assets/91594726-fb45-43b8-a67e-3a24ab11564d)


### 图18 - 13 PEFT

**PEFT**

**State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods**

Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. Fine-tuning large-scale PLMs is often prohibitively costly. In this regard, PEFT methods only fine-tune a small number of (extra) model parameters, thereby greatly decreasing the computational and storage costs. Recent State-of-the-Art PEFT techniques achieve performance comparable to that of full fine-tuning. 

PEFT技术旨在通过最小化微调参数的数量和计算复杂度来提高预训练模型在新任务上的性能，从而缓解大型预训练模型的训练成本。这样一来，即使计算资源受限，也可以利用预训练模型的知识来迅速适应新任务，实现高效的迁移学习。

因此，PEFT技术可以在提高模型效果的同时，大大缩短模型训练时间和降低计算成本，让更多人能够参与到深度学习的研究中来。

#### 18.4.1 PEFT技术详解

前面我们已经讲过了，对于训练和开发成本达到上亿美元的ChatGLM来说，大型预训练模型的训练成本非常高昂，需要庞大的计算资源和大量的数据，一般人难以承受。这也导致了一些研究人员难以重复和验证先前的研究成果。为了解决这个问题，研究人员开始研究PEFT技术。



相较于前面介绍的LoRA调参方法，PEFT设计了3种结构，将其嵌入Transformer结构中。

- **Adapter-Tuning**：将较小的神经网络层或模块插入预训练模型的每一层，这些新插入的神经模块称为Adapter（适配器），下游任务微调时只训练这些适配器参数。

- **LoRA**：通过学习小参数的低秩矩阵来近似模型权重矩阵的参数更新，训练时只优化低秩矩阵参数。

- **Prefix/Prompt-Tuning**：在模型的输入层或隐藏层添加若干额外可训练的前缀Tokens（这些前缀是连续的伪Tokens，不对应真实的Tokens），只训练这些前缀参数即可。

![image](https://github.com/user-attachments/assets/495f9216-8a9f-4cc2-a936-978bbac26629)


LoRA结构前面已经介绍过了，这里不再重复讲解。下面以Adapter结构为例进行介绍，Adapter结构如图18 - 14所示。

![image](https://github.com/user-attachments/assets/0c23d137-f179-4c21-9064-fe3949f87596)


### 图18 - 14 Adapter结构

在训练时，原来预训练模型的参数固定不变，只对新增的Adapter结构进行微调。同时为了保证训练的高效性（也就是尽可能少的引入更多参数），Adapter结构设计如下：

- 一个down-project层将高维特征映射到低维特征，之后经过一个非线性计算层对数据进行低维计算，再用一个up-project结构将低维特征映射回原来的高维特征。

- 同时也设计了skip-connection结构，确保在最差的情况下能够退化为identity。

Prefix-Tuning结构在模型输入前会添加一个连续且任务特定的向量序列（Continuous Task-Specific Vector），称之为前缀（Prefix）。前缀被视为一系列虚拟Tokens，它由不对应真实Tokens的自由参数组成。与更新所有PLM参数的全量微调不同，Prefix-Tuning固定PLM的所有参数，只更新优化特定任务的前缀。因此，在生产部署时，只需要存储一个大型PLM的副本和一个学习到特定任务的前缀，每个下游任务就只会产生非常小的额外计算和存储开销。


#### 18.4.2 PEFT的使用与参数设计

首先来看PEFT的使用方法，官方既定的PEFT演示内容如下：

```python
from transformers import AutoModelForSeq2SeqLM
from peft import get_peft_config, get_peft_model, LoraConfig, TaskType

peft_config = LoraConfig(
    task_type=TaskType.MODEL_TYPE,
    inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
    target_modules = ["query_key_value"]
)

model = "加载的模型"
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
```

这里首先通过peft_config设定了加载PEFT的参数，之后将加载的模型与PEFT参数通过给定的get_peft_model函数进行结合。


下面我们观察一下PEFT的config文件，其中对相关参数进行了定义，内容如下：

```json
{
    "base_model_name_or_path": null,
    "bias": "none",
    "enable_lora": [
        true,
        false,
        true
    ],
    "fan_in_fan_out": true,
    "inference_mode": false,
    "lora_alpha": 32,
    "lora_dropout": 0.1,
    "merge_weights": false,
    "modules_to_save": null,
    "peft_type": "LORA", #PEFT类型
    "r": 8,
    "target_modules": [
        "query_key_value"
    ],
    "task_type": "CAUSAL_LM"
}
```
可以看到，其中最关键的是定义了peft_type，这里为了便于比较，同样采用LoRA方法对模型进行微调。将其引入我们自定义的ChatGLM模型中（这里无法使用原有的自定义ChatGLM模型，参考18.4.3节），这部分代码如下：
```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = "0"
import torch
from tqdm import tqdm
from torch.utils.data import RandomSampler, DataLoader
from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_int8_training, get_peft_model_state_dict
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoConfig

device = "cuda"
from 第十八章.chatGLM_spo.fintunning_peft_xiaohua import modeling_chatglm
model = modeling_chatglm.XiaohuaModel(model_path="../huggingface_saver/chatglm6b.pth")
peft_config = LoraConfig.from_pretrained("./peft")
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
model = model.half().to(device)  # .to(device)
```

打印结果如图18 - 15所示。

![image](https://github.com/user-attachments/assets/4d38544d-c088-4d45-8433-0babd218b18a)


### 图18 - 15 打印结果
```
trainable params: 3670016 || all params: 6711599104 || trainable%: 0.05468169274015089
```
可以很明显地看到，此时同样冻结了绝大部分参数，而可训练的参数只占全部参数的5%左右。

#### 18.4.3 Huggingface专用PEFT的使用
本小节讲解PEFT的使用，对于Huggingface发布的PEFT库，我们只需要遵循其使用规则，直接将其加载到原有模型类中即可。

由于PEFT库是由Huggingface发布的，其天然适配Huggingface的输入输出接口，因此如果想要将其加载到我们自定义的ChatGLM代码段中，需要遵循PEFT对接口的定义标注，即需要在原有的model类中定义Huggingface标准的forward函数，如图18 - 16所示。

![image](https://github.com/user-attachments/assets/34155aa1-850a-4698-b061-086a6bd35e35)


### 图18 - 16 Huggingface标准的forward函数
```python
def forward(self,input_ids,labels = None,position_ids = None,attention_mask = None):
    logits,hidden_states = self.glm_model.forward(input_ids=input_ids,position_ids = None,attention_mask = None)
    loss = None
    if labels is None:
        shift_logits = logits[:, :-1].contiguous()
        shift_labels = input_ids[:, 1:].contiguous()
        # Flatten the tokens
        logits_1 = shift_logits.view(-1, shift_logits.size(-1))
        logits_2 = shift_labels.view(-1)
        loss = self.loss_fct(logits_1, logits_2)
    return logits,hidden_states,loss
```
图中代码重新适配了新的forward函数，从而完成对PEFT库的加载，如图18 - 17所示。

![image](https://github.com/user-attachments/assets/d05bc5ab-c879-4f53-b584-10ca3be4ee4e)


### 图18 - 17 重新适配了新的forward函数


```python
def forward(self,input_ids,attention_mask = None,input_embeds = None,labels = None,hidden_states = None,output_attentions = None,output_hidden_states = None,return_dict = None,**kwargs):
    logits,hidden_states = self.glm_model.forward(input_ids=input_ids,**kwargs)
    shift_logits = logits[:, :-1].contiguous()
    shift_labels = input_ids[:, 1:].contiguous()
    # Flatten the tokens
    logits_1 = shift_logits.view(-1, shift_logits.size(-1))
    logits_2 = shift_labels.view(-1)
    loss = self.loss_fct(logits_1, logits_2)
    return logits,hidden_states,loss
```
大部分参数在模型的计算过程中并没有使用，这些只是为了适配PEFT模型而设置的虚参数。

完整地使用PEFT对自定义的模型进行微调的代码如下：
```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = "0"
import torch
from tqdm import tqdm
from torch.utils.data import RandomSampler, DataLoader
from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_int8_training, get_peft_model_state_dict
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoConfig

device = "cuda"
from 第十八章.chatGLM_spo.fintunning_peft_xiaohua import modeling_chatglm
model = modeling_chatglm.XiaohuaModel(model_path="../huggingface_saver/chatglm6b.pth")
peft_config = LoraConfig.from_pretrained("./peft")
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
model = model.half().to(device)

prompt_text = "按给定的格式抽取文本信息。\n文本："
from 第十八章.chatGLM_spo import get_data
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
all_train_data = get_data.get_train_data("../data/spo_0.json", tokenizer, 48, 48, prompt_text)
train_dataset = get_data.Seq2SeqDataSet(all_train_data)
train_loader = DataLoader(train_dataset, batch_size=2, drop_last=True, collate_fn=get_data.coll_fn, num_workers=0)

from accelerate import Accelerator
accelerator = Accelerator()

optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-5)
lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2400, eta_min=2e-6, last_epoch=-1)

model, optim, train_loader, lr_scheduler = accelerator.prepare(model, optimizer, train_loader, lr_scheduler)

for epoch in range(20):
    pbar = tqdm(train_loader, total=len(train_loader))
    for batch in (pbar):
        input_ids = batch["input_ids"].cuda()
        labels = batch["labels"].cuda()
        _,_,loss = model.forward(input_ids, labels=labels)
        accelerator.backward(loss)
        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)
        optimizer.step()
        lr_scheduler.step()  # 执行优化器
        optimizer.zero_grad()
        pbar.set_description(
            f"epoch:{epoch + 1}, train_loss:{loss.item():.5f}, lr:{lr_scheduler.get_last_lr()[0] * 1000:.5f}"
        )
        if (epoch +1) % 3 == 0:
            torch.save(model.state_dict(), "./glm6b_peft.pth")
```
训练过程请读者自行验证。

对于模型的推断，读者可以直接保存全部参数，在推断时重新加载PEFT包之后再载入全部参数。

### 18.5 本章小结
本章主要介绍对ChatGLM模型进行本地化的方法，举例说明了根据不同数据对ChatGLM进行微调的方法。对于具体的微调选择来说，不存在基于任务的最佳微调方法，但在一些特定场景下，会有一种较好的方法。例如，LoRA在低/中资源的场景下表现最好，而完全微调在我们增加数据量到更高的样本时，相对性能会增加。速度和性能之间存在着明显的区别，PEFT的速度更差，但在低资源下的性能更好，而随着数据量的增加，性能的提升也更为明显。具体测试还请读者自行根据需要落地的业务对微调进行处理。

大模型是深度学习自然语言处理皇冠上的一颗明珠，本书只是起到抛砖引玉的作用，大模型的更多技术内容还需要读者在实践中持续学习和研究。 
