
### 18.2.2 加速的秘密——Accelerate模型加速工具详解

Accelerate是Huggingface开源的、方便将PyTorch模型在不同模式下进行训练的一个小巧工具。



与标准的PyTorch方法相比，使用Accelerate进行GPU、多GPU以及半精度FP16/BF16训练时，模型的训练过程变得非常简单（只需要在标准的PyTorch训练代码中改动几行代码，就可以适应CPU/单GPU/多GPU的DDP模式/TPU等不同的训练环境），而且速度与原生PyTorch相当，非常快。



Accelerate的使用相当简单，可以直接在模型训练代码中对模型的训练函数进行更新，代码如下：

```python
...
from accelerate import Accelerator
accelerator = Accelerator()

optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-5)
lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2400, eta_min=2e-6, last_epoch=-1)
model, optim, train_loader, lr_scheduler = accelerator.prepare(model, optimizer, train_loader, lr_scheduler)
...
with train:
    accelerator.backward(loss)
```

在上面代码中，首先从accelerate导入加速器的具体实现Accelerator，之后用实例化后的Accelerator对模型的主体部分、优化器、数据更新器以及学习率等内容进行调整，最后的backward函数的作用是为向后传递添加必要的步骤来提高混合精度。

完整的模型训练方法如下：

```python
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

import torch
from transformers import AutoTokenizer
from torch.utils.data import RandomSampler, DataLoader
from 第十八章.chatGLM_spo.huggingface_saver import xiaohua_model, configuration_chatglm, modeling_chatglm
from tqdm import tqdm

config = configuration_chatglm.ChatGLMConfig()
config.pre_seq_len = 16
config.prefix_projection = False
# 这里是设置config中的pre_seq_len与prefix_projection，只有这2项设置好了才行

model = xiaohua_model.XiaohuaModel(model_path="../huggingface_saver/chatglm6b.pth", config=config, strict=False)
model = model.half().cuda()

xiaohua_model.print_trainable_parameters(model)
prompt_text = "按给定的格式抽取文本信息。\n文本："
from 第十八章.chatGLM_spo import get_data
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
all_train_data = get_data.get_train_data("../data/spo_0.json", tokenizer, 288, 256, prompt_text)
train_dataset = get_data.Seq2SeqDataSet(all_train_data)
train_loader = DataLoader(train_dataset, batch_size=2, drop_last=True, collate_fn=get_data.coll_fn, num_workers=0)

from accelerate import Accelerator
accelerator = Accelerator()

optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-5)
lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2400, eta_min=2e-6, last_epoch=-1)
model, optim, train_loader, lr_scheduler = accelerator.prepare(model, optimizer, train_loader, lr_scheduler)

for epoch in range(20):
    pbar = tqdm(train_loader, total=len(train_loader))
    for batch in (pbar):
        input_ids = batch["input_ids"].cuda()
        labels = batch["labels"].cuda()

        _, loss = model.forward(input_ids, labels=labels)
        accelerator.backward(loss)
        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)
        optimizer.step()  # 执行优化器
        lr_scheduler.step()
        optimizer.zero_grad()

        pbar.set_description(f"epoch:{epoch + 1}, train_loss:{loss.item():.5f}, lr:{lr_scheduler.get_last_lr()[0] * 1000:.5f}")
        if (epoch + 1) % 3 == 0:
            torch.save(model.state_dict(), "../glm6b_pt.pth")
```
上面代码的最后一行将全部内容保存为.pth后缀的标准PyTorch模型存档文件，在使用时可以将其视为普通的PyTorch文件进行载入。


```python
model = xiaohua_model.XiaohuaModel(model_path="../huggingface_saver/chatglm6b.pth", config=config, strict=False)
model.load_state_dict(torch.load("../glm6b_pt.pth"))
model = model.half().cuda()
```


可以看到，在模型进行基本的载入后，又重新载入了我们独立进行微调（Fine-Tuning）后保存的模型参数。完整使用存档参数进行推断的函数代码如下：



```python
def sample_top_p(probs, p):
    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)
    probs_sum = torch.cumsum(probs_sort, dim=-1)
    mask = probs_sort > p
    probs_sort[mask] = 0.0
    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))
    next_token = torch.multinomial(probs_sort, num_samples=1)
    next_token = torch.gather(probs_idx, -1, next_token)
    return next_token

import torch
from transformers import AutoTokenizer
from torch.utils.data import RandomSampler, DataLoader
from 第十八章.chatGLM_spo.huggingface_saver import xiaohua_model, configuration_chatglm, modeling_chatglm
from tqdm import tqdm

config = configuration_chatglm.ChatGLMConfig()
config.pre_seq_len = 16
config.prefix_projection = False
# 这里是设置config中的pre_seq_len与prefix_projection，只有这2项设置好了才行

model = xiaohua_model.XiaohuaModel(model_path="../huggingface_saver/chatglm6b.pth", config=config, strict=False)
model.load_state_dict(torch.load("../glm6b_pt.pth"))
model = model.half().cuda()

xiaohua_model.print_trainable_parameters(model)
model.eval()
max_len = 288
max_src_len = 256
prompt_text = "按给定的格式抽取文本信息。\n文本："
save_data = []
fl = 0.0
max_tgt_len = max_len - max_src_len - 3
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
import time, json
s_time = time.time()
with open("../data/spo_0.json", "r", encoding="utf-8") as fh:
    for i, line in enumerate(tqdm(fh, desc="iter")):
        with torch.no_grad():
            sample = json.loads(line.strip())
            src_tokens = tokenizer.tokenize(sample["text"])
            prompt_tokens = tokenizer.tokenize(prompt_text)
            if len(src_tokens) > max_src_len - len(prompt_tokens):
                src_tokens = src_tokens[:max_src_len - len(prompt_tokens)]
            tokens = prompt_tokens + src_tokens + ["[gMASK]", "<sop>"]
            input_ids = tokenizer.convert_tokens_to_ids(tokens)
            # input_ids = tokenizer.encode("帮我写个快排算法")
            for _ in range(max_src_len):
                input_ids_tensor = torch.tensor(input_ids).to("cuda")
                logits, _, _ = model.forward(input_ids_tensor)
                logits = logits[:, -3:]
                probs = torch.softmax(logits / 0.95, dim=-1)
                next_token = sample_top_p(probs, 0.95)  # 预设的top_p = 0.95
                # next_token = next_token.reshape(-1)
                # next_token = result_token[-3:-2]
                input_ids = input_ids[:-2] + [next_token.item()] + input_ids[-2:]
                if next_token.item() == 130005:
                    print("break")
                    break
                result = tokenizer.decode(input_ids)
                print(result)
```
上面实现请读者自行训练并验证。

### 18.2.3 更快的速度——使用INT8（INT4）量化模型加速训练
本小节介绍模型的数据类型，前面进行模型推断使用的都是half()函数，这是PyTorch特有的进行半精度训练的参数方式，即可以在“略微”降低模型准确率的基础上大幅度减少硬件的消耗，具体读者可以参考表18-1。

### 表18-1 显存占用表


| 量化等级 | 最低GPU显存（推理） | 最低GPU显存（高效参数微调） |
| ---- | ---- | ---- |
| Half（无量化） | 13 GB | 16 GB |
| INT8 | 8 GB | 9 GB |
| INT4 | 6 GB | 7 GB |

表中的Half数据类型为PyTorch 2.0中的半精度数据类型，在降低参数占用空间的同时，对准确率影响较小。而其使用也较为简单，即直接在PyTorch模型构建时显式注释即可：


```python
model = xiaohua_model.XiaohuaModel(model_path="../huggingface_saver/chatglm6b.pth", config=config, strict=False)
model = model.half().cuda()
```
除此之外，表18-1中还列出了两种模型参数的格式，分别是INT8与INT4参数格式。这是一种为了解决大模型参数量占用过大而提出的一种加速推理的技术。

以INT8量化为例，相比于一般的FLOAT32模型，该模型的大小减小了4倍，内存要求减少2倍。与FLOAT32计算相比，对INT8计算的硬件支持通常快2 - 4倍。大多数情况下，模型需要以FLOAT32精度训练，然后将模型转换为INT8。

有兴趣的读者可以自行学习基于PyTorch大模型的量化方法，在这里提供了如下几种可以缩减计算量的模型量化方案。

1. **基于清华大学提供的模型量化存档**

清华大学在提供ChatGLM存档文件的同时，也相应地提供了INT8和INT4参数的下载方法，读者可以在下载时直接指定需要下载的量化类型，代码如下：

```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b-int4", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm-6b-int4", trust_remote_code=True).half().cuda()
...
```

在这里直接下载了INT4文件进行后续的推断与处理，这样做的好处在于下载的文件较小，但同时伴随着对预测精度的牺牲，读者可以自行下载文件，比较推断结果。

2. **基于模型文件的量化方法**

前面18.1.1节演示了如何直接下载ChatGLM的构成文件，在其定义的ChatGLM类中，同时提供了相对应的量化方法，代码如下：

```python

def quantize(self, bits: int, empty_init=False, **kwargs):

    if bits == 0:
        return
    from.quantization import quantize
    if self.quantized:
        return self
    self.quantized = True
    self.config.quantization_bit = bits
    self.transformer = quantize(self.transformer, bits, empty_init=empty_init, **kwargs)
``` 
